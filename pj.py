import os
# Définir le chemin absolu du répertoire vers lequel vous souhaitez changer
nouveau_repertoire = 'C:/Users/jmv/Documents/Travail/Master 2/methodes app/Projet'

# Utiliser os.chdir pour changer de répertoire
os.chdir(nouveau_repertoire)
repertoire_actuel = os.getcwd()
print("Répertoire actuel :", repertoire_actuel)
##
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import statistics
import seaborn as sns

data=pd.read_csv('Data.csv')
print((data['Label'] == 'malware').sum(),(data['Label'] == 'goodware').sum())
#la somme fait 4464 et pas 4465, il y'a une ligne de nan, il faut la supprimer
data=data.drop(2533)
print(data.shape)
data=data.drop_duplicates()
print(data.shape)
##
data[data['Label'] == 'malware']['ACCESS_NETWORK_STATE'].mean()
n1=data[data['Label'] == 'malware']['RECEIVE_BOOT_COMPLETED'].mean()
n2=data[data['Label'] == 'goodware']['RECEIVE_BOOT_COMPLETED'].mean()
print(f'Moyenne RECEIVE_BOOT_COMPLETD malware : {n1}, goodware : {n2}') #la plupart des malwares ont cette variable

##
X=data.drop(['Label'],axis=1)
Y=data['Label']

correlation_matrix = data.corr()

# Créer une représentation graphique de la matrice de corrélation sous forme de heatmap avec Matplotlib
plt.figure(figsize=(10, 4))
plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.title('Matrice de corrélation')
plt.tight_layout()
plt.show()

Y = Y.replace({'malware': 1, 'goodware': 0})
##
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)

##reg logistique
from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0).fit(X_train, Y_train)
Y_pred0=log.predict(X_train)
print((Y_pred0 != Y_train).mean())
Y_pred1=log.predict(X_test)
print((Y_pred1 != Y_test).mean())

from sklearn.metrics import confusion_matrix
conf = confusion_matrix(Y_test, Y_pred1)
sns.heatmap(conf, annot=True)
plt.show()

m=X.shape[1]
odds=np.zeros(m)
for i in range(0,m):
    odds[i]=np.exp(log.coef_[0][i])
print(np.where(odds>5)) #malwares plus fréquents dans ces variables
print(np.where(odds<0.1)) #malwares moins fréquents dans ces variables
#exemple
print(data.columns[182])
print(data[data['Label'] == 'malware'][data.columns[182]].mean())
print(data[data['Label'] == 'goodware'][data.columns[182]].mean())

##kpp
#classe malware majoritaire, c'est un problème pour les kpp
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

err_knn = []
err_knn_in = []
Kvoisins = []
for k in  range(10):
    knn = KNeighborsClassifier(n_neighbors = 2*k+1)
    knn.fit(X_train, Y_train)
    conf_knn = confusion_matrix(Y_test, knn.predict(X_test))
    conf_knn2 = confusion_matrix(Y_train, knn.predict(X_train))

    err_knn.append ((conf_knn[1,0] +  conf_knn[0,1])/1116)
    err_knn_in.append ((conf_knn2[1,0] +  conf_knn2[0,1])/3348)
    Kvoisins.append(2*k+1)

print(f"Le k qui minimise l'erreur de prévision est {Kvoisins[np.argmin(err_knn)]}")

k_best=Kvoisins[np.argmin(err_knn)]
err_min=err_knn[k_best]

plt.figure(figsize=[8,4])
plt.plot(Kvoisins, err_knn, 'k*-',label='Erreur test')
plt.plot(Kvoisins, err_knn_in, 'r*-',label='Erreur training')
plt.xlabel("Nombre de Voisins")
plt.ylabel("Erreur de prévision Test")
plt.legend()
plt.grid()
plt.plot()
plt.show()

##
neigh = KNeighborsClassifier(n_neighbors=k_best)
neigh.fit(X_train, Y_train)
print((neigh.predict(X_test) != Y_test).mean())
#à voir avec des classes équilibrées plus tard

##
from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors': [2*k+1 for k in range(0,10)]}
# La fonction GridSearchCV automatise la recherche d’un optimum parmi les hyperparamètre, elle utilise notamment la validation croisée. On teste toutes les valeurs de k de 1 à 20.
knn = KNeighborsClassifier()
grid = GridSearchCV(knn, parameters, verbose=2, return_train_score=True,cv=5)
grid.fit(X_train, Y_train)


grid.best_params_
##
from sklearn.tree import DecisionTreeClassifier, plot_tree
tree = DecisionTreeClassifier(max_depth=30) # par défaut c'est l'algorithme CART qui est implémenté
tree.fit(X_train, Y_train)

conf = confusion_matrix(Y_test, tree.predict(X_test))
sns.heatmap(conf, annot=True)
plt.show()

plot_tree(tree)
plt.show()

##
parameters = {'max_depth': np.arange(1,31)}

tree = DecisionTreeClassifier()
clf = GridSearchCV(tree, parameters, cv = 10)
clf.fit(X_train, Y_train)

print(clf.best_estimator_)


print((clf.predict(X_test)!= Y_test).mean())

conf = confusion_matrix(Y_test, clf.predict(X_test))
sns.heatmap(conf, annot=True)
plt.show()
##
from sklearn.metrics import roc_curve, roc_auc_score, auc

# Obtenez les probabilités prédites pour chaque modèle
y_pred_prob1 = log1.predict_proba(X_test)[:, 1]
y_pred_prob2 = gridd.predict_proba(X_test)[:, 1]
y_pred_prob3 = clff.predict_proba(X_test)[:, 1]
#y_pred_prob4 = lr3.predict_proba(dataTest_x)[:, 1]
# Calculez les courbes ROC pour chaque modèle
fpr1, tpr1, _ = roc_curve(Y_test, y_pred_prob1)
fpr2, tpr2, _ = roc_curve(Y_test, y_pred_prob2)
fpr3, tpr3, _ = roc_curve(Y_test, y_pred_prob3)

# Calculez l'AUC pour chaque modèle
roc_auc1 = auc(fpr1, tpr1)
roc_auc2 = auc(fpr2, tpr2)
roc_auc3 = auc(fpr3, tpr3)
# Tracez les courbes ROC sur un même graphique
plt.figure(figsize=(8, 6))
plt.plot(fpr1, tpr1, color='darkorange', lw=2, label=f'Modèle 1 (AUC = {roc_auc1:.2f})')
plt.plot(fpr2, tpr2, color='blue', lw=2, label=f'Modèle 2 (AUC = {roc_auc2:.2f})')
plt.plot(fpr3, tpr3, color='green', lw=2, label=f'Modèle 3 (AUC = {roc_auc3:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de faux positifs')
plt.ylabel('Taux de vrais positifs')
plt.title('Courbes ROC de différents modèles')
plt.legend(loc='lower right')
plt.show()
##
plt.hist(log.predict_proba(X_test)[:, 1],bins=180,density=True)
plt.hist(log.predict_proba(X_test)[:, 0],bins=180,density=True,color='red')

plt.show()
##
from imblearn.under_sampling import NearMiss
nm = NearMiss(sampling_strategy='auto')
X_res, Y_res = nm.fit_resample(X, Y)
##
log1 = LogisticRegression(random_state=0).fit(X_res, Y_res)
plt.hist(log1.predict_proba(X_test)[:, 1],bins=180,density=True)
plt.hist(log1.predict_proba(X_test)[:, 0],bins=180,density=True,color='red')

plt.show()
##
from imblearn.over_sampling import SMOTENC
smote_nc = SMOTENC(sampling_strategy='auto', categorical_features=[1])
X_ress, Y_ress = smote_nc.fit_resample(X, Y)
##
log2 = LogisticRegression(random_state=0).fit(X_ress, Y_ress)
plt.hist(log2.predict_proba(X_test)[:, 1],bins=180,density=True)
plt.hist(log2.predict_proba(X_test)[:, 0],bins=180,density=True,color='red')
plt.show()
##
parameters = {'n_neighbors': [2*k+1 for k in range(0,10)]}
gridd = GridSearchCV(knn, parameters, verbose=2, return_train_score=True)
gridd.fit(X_res, Y_res)
gridd.best_estimator_
##
parameters = {'max_depth': np.arange(1,31)}

tree = DecisionTreeClassifier()
clff = GridSearchCV(tree, parameters, cv = 5)
clff.fit(X_res, Y_res)
##
from sklearn.ensemble import RandomForestClassifier
forest0 = RandomForestClassifier(n_estimators=1)
forest1 = RandomForestClassifier(n_estimators=5)
forest2 = RandomForestClassifier(n_estimators=20)
forest3 = RandomForestClassifier(n_estimators=100)
forest0.fit(X_res,Y_res)
forest1.fit(X_res,Y_res)
forest2.fit(X_res,Y_res)
forest3.fit(X_res,Y_res)
##
y_pred_prob1 = log1.predict_proba(X_test)[:, 1]
y_pred_prob2 = gridd.predict_proba(X_test)[:, 1]
y_pred_prob3 = clff.predict_proba(X_test)[:, 1]
y_pred_prob4 = forest0.predict_proba(X_test)[:, 1]
y_pred_prob5 = forest1.predict_proba(X_test)[:, 1]
y_pred_prob6 = forest2.predict_proba(X_test)[:, 1]
y_pred_prob7 = forest3.predict_proba(X_test)[:, 1]

#y_pred_prob4 = lr3.predict_proba(dataTest_x)[:, 1]
# Calculez les courbes ROC pour chaque modèle
fpr1, tpr1, _ = roc_curve(Y_test, y_pred_prob1)
fpr2, tpr2, _ = roc_curve(Y_test, y_pred_prob2)
fpr3, tpr3, _ = roc_curve(Y_test, y_pred_prob3)
fpr4, tpr4, _ = roc_curve(Y_test, y_pred_prob4)
fpr5, tpr5, _ = roc_curve(Y_test, y_pred_prob5)
fpr6, tpr6, _ = roc_curve(Y_test, y_pred_prob6)
fpr7, tpr7, _ = roc_curve(Y_test, y_pred_prob7)

# Calculez l'AUC pour chaque modèle
roc_auc1 = auc(fpr1, tpr1)
roc_auc2 = auc(fpr2, tpr2)
roc_auc3 = auc(fpr3, tpr3)
roc_auc4 = auc(fpr4, tpr4)
roc_auc5 = auc(fpr5, tpr5)
roc_auc6 = auc(fpr6, tpr6)
roc_auc7 = auc(fpr7, tpr7)

# Tracez les courbes ROC sur un même graphique
plt.figure(figsize=(8, 6))
plt.plot(fpr1, tpr1, color='darkorange', lw=2, label=f'Modèle 1 (AUC = {roc_auc1:.2f})')
plt.plot(fpr2, tpr2, color='blue', lw=2, label=f'Modèle 2 (AUC = {roc_auc2:.2f})')
plt.plot(fpr3, tpr3, color='green', lw=2, label=f'Modèle 3 (AUC = {roc_auc3:.2f})')
plt.plot(fpr4, tpr4, color='m', lw=2, label=f'Modèle 4 (AUC = {roc_auc4:.2f})')
plt.plot(fpr5, tpr5, color='y', lw=2, label=f'Modèle 5 (AUC = {roc_auc5:.2f})')
plt.plot(fpr6, tpr6, color='r', lw=2, label=f'Modèle 6 (AUC = {roc_auc6:.2f})')
plt.plot(fpr7, tpr7, color='c', lw=2, label=f'Modèle 7 (AUC = {roc_auc7:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de faux positifs')
plt.ylabel('Taux de vrais positifs')
plt.title('Courbes ROC de différents modèles')
plt.legend(loc='lower right')
plt.show()

